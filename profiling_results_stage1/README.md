* wrk2_t2_c100_d10s_R2000_results:
Демонстрирует обработку нагрузки 2000 запросов в сек.
Стандартное отклонение довольно большое: 75.54%.

* wrk2_t2_c100_d10s_R2000_latency_results:
Демонстрирует распределение времен ответа.
Видно, что wrk2 поддерживает Request rate, который мы просили.
Показывает, что 99% времен ответа уложилось в 3.10 мс.

* img0.png, img1.png и img2.png:
Демонстрирует мониторинг нагрузки с помощью инструмента Visual VM.


- Результаты профилирования под нагрузкой:
* cpu.svg:
Профиль потребления CPU.
Selector фигурировал в 100% семплов CPU.
56.25% CPU ушло на Session.process - обработка HTTP-запроса.
Затем парсинг буфера.
Затем запись результатов в Socket (8.33%).
Т.о. мы видим, как CPU тартится на сетевой ввод-вывод.

* alloc.svg:
Результаты аллокации.
Найдем код, который больше всего аллоцирует памяти:
Обрабатывает HTTP-буфер, который накопили (98.89%).
Парсит сам запрос (31.11%).
Затем вызывается обработчик handleRequest (52.22%).
Затем отправляет ответ (22.22%).
Responce преобразует в байты, при этом выделяет массив байт (18.89%).
И создает Response.ok (30.00%), который содержит массив байт (6.67%).
Здесь представляется возможной следующая оптимизация: закэшировать Responce.ok и всегда его возвращать (на этом можно будет сэкономить 30.00%).

* cpu_put_with_10-connections.svg:
Демонстрация сломанного TreeMap при запуске скрипта wrk: put.lua с 10 соединениями.
Поэтому на данном этапе необходимо стрелять через одно соединение.

* cpu_put_with_1-connections.svg:
Теперь стреляем в один поток PUT'ами.
Компилятор работает. Сеть занимает время (61.39%).
Логгер ничего не потребляет, так как в реализации убран _logger.debug_ .

* wrk_t1_c1_d10m_s_wrk_put-lua_latency_localhost-8080
Видно, что обрабатывает 14319.50 запросов в секунду.
Видно, что 90% за 56.00us и 99% за 40.04ms.
Можно наблюдать мониторинг в Visual VM на рисунке img3.png.
На 9% тратится столько времени, так как мы при достижении порога по памяти в нашей реализации сериализуем таблицу на диск и делаем это синхронно, то есть блокируем запрос, пока не запишем на диск. Поэтому такие всплески, как видно на рисунке (то есть flush необходимо сделать асинхронно).
Всего было выполнено 3873721 запросов за 4.51 мин.

* wrk_t1_c1_d10m_s_wrk_get-lua_latency_localhost-8080
Теперь стреляем в один поток GET'ами.
Результат профилирования можно увидеть на cpu_get_with_1connections.svg.
Также можно видеть, что 11512.12 запросов в секунду и обработано 1549744 запросов за 2.24 мин.
Результат мониторинга можно видеть на img4.png.

